# 회귀

## 220324

| 독립변수 개수      | 회귀 계수위 결합     |
| ------------------ | -------------------- |
| 1개 : 단일 회귀    | 선형 : 선형 회귀     |
| 여러개 : 다중 회귀 | 비선형 : 비선형 회귀 |

* 대표적인 선형 회귀 모델

  * 일반 선형 회귀
  * 릿지
  * 라쏘
  * 엘라스틱넷
  * 로지스틱 회귀(분류)

* 잔차 : 실제 값과 회귀 모델의 차이에 따른 오류 값 ->최적의 회귀는 이 잔차를 최소가 되는 모델을 만든다는 의미

* 오류의 값은 +나 -가 될 수 있기때문에 오류의 합을 구하기 위해 단순 덧셈뺄셈은 뜻하지 않게 오류 합이 크게 줄어들 수 있음 -> RSS 사용

* RSS : 오류 값의 제곱을 구해서 더하는 방식
  $$
  RSS = Error^2
  $$

* 회귀에서 이 RSS는 비용(Cost)이며 w 변수로 구성되는 RSS를 비용함수(손실함수)라고 함

$$
RSS(w_0,w_1) = \frac{1}{N}\displaystyle\sum_{i=1}^{N} (y_i-(w_0+w_1*x_i))^2
$$

### 경사 하강법

* 경사하강법(Gradient Descent)은 1차 근삿값 발견용 **최적화 알고리즘**이다. 기본 개념은 함수의 기울기(경사)를 구하고 경사의 절댓값이 낮은 쪽으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것이다.
* 딥러닝에 있어서 가장 중요한 개념중 하나

### 다중공선성

* 피쳐간의 상관관계가 매우 높은 경우 분산이 매우 커져 오류에 매우 민감해지는 경우
* 일반적으로 상관관계가 높은 피쳐가 많은 경우 독립적인 중요한 피쳐만 남기고 제거하거나 규제를 적용
* 매우 많은 피쳐가 다중 공선성 문제를 가지고 있다면 pca를 통해 차원축소를 진행할수도 있음

### 회귀 평가 지표

* MAE : mean absolute error이며 실제 값과 예측값의 차이를 절댓값으로 변환해 평균한 것
* MSE : mean squared error이며 실제값과 예측값의 차이를 제곱해 평균한것
* RMSE : MSE에 루트를 씌운것(MSE는 실제 오류 평균보다 더 커지는 특성이 있음)
* R^2 : 분산 기반 예측성능 평가. 1에 가까울 수록 예측정확도 높음

## 220329

### 다항회귀와 과(대)적합/과소적합 이해

#### 다항회귀

* 회귀가 독립변수의 단항식이 아닌 2차, 3차 방정식과 같은 다항식으로 표현되는것을 다항회귀라고 한다
* 다항회귀를 비선형회귀로 혼동하기 쉬우나 다항회귀는 선형회귀임
  * 회귀에서 선형/비선형을 나누는 기준은 회귀계수가 선형/비선형인지에 따른것이지 독립변수의 선형/비선형 여부와는 무관함

#### 다항회귀를 이용한 과소적합 및 과적합 이해

* 



